# Evaluating the Practicality of Counterfactual Explanations

Machine learning models are increasingly used for decisions that directly affect peopleâ€™s lives. These models are often a black box, meaning that the people affected cannot understand how or why the decision was made. However, according to the General Data Protection Regulation, they have the right to an explanation. Coun- terfactual explanations are a way to make machine learning models more transparent by showing how attributes need to be changed to get a different result. This type of explanation is considered easy to understand and human-friendly. To be used in real life, explana- tions must be practical, which means they must go beyond a purely theoretical framework. Research has focused on defining several objective functions to compute practical counterfactuals. However, it has not yet been tested whether people perceive the explanations as such in practice. To address this, we contribute by identifying properties that explanations must satisfy to be practical for human subjects. The properties are then used to evaluate the practicality of two counterfactual methods by conducting a user study. The study aims to compare the original approach to compute counterfactual explanations to a more recent framework. The results show that human subjects perceive the more recent method as more practical.

Contains key documentations of my masterthesis. Includes code for computing counterfactual explanations, layout of user study and analysis code of the study results. 
